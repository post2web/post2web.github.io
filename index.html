<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is my data science blog.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Ivelin's TL;DR Data Science Blog (page 1) | Ivelin's TL;DR Data Science Blog</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://post2web.github.io/">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-113338661-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113338661-1');
</script><link rel="prefetch" href="posts/word2vec-with-tensorflow/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://post2web.github.io/">

                <span id="blog-title">Ivelin's TL;DR Data Science Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="active">
<a href=".">Blog <span class="sr-only">(active)</span></a>
                </li>
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/word2vec-with-tensorflow/" class="u-url">Word2Vec with TensorFlow</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Ivelin Angelov
            </span></p>
            <p class="dateline"><a href="posts/word2vec-with-tensorflow/" rel="bookmark"><time class="published dt-published" datetime="2018-01-20T13:00:07-08:00" title="2018-01-20 13:00">2018-01-20 13:00</time></a></p>
                <p class="commentline">
        
    <a href="posts/word2vec-with-tensorflow/#disqus_thread" data-disqus-identifier="cache/posts/word2vec-with-tensorflow.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Word2Vec-with-Skip-Gram-and-TensorFlow">Word2Vec with Skip-Gram and TensorFlow<a class="anchor-link" href="posts/word2vec-with-tensorflow/#Word2Vec-with-Skip-Gram-and-TensorFlow">¶</a>
</h2>
<p>This is a tutorial and a basic example for getting started with word2vec model by <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al</a>. It is used for learning vector representations of words, called "Words Embeddings". For more information about Embeddings, read my previous post.</p>
<h4 id="The-word2vec-model-can-be-trained-with-two-different-word-representations:">The word2vec model can be trained with two different word representations:<a class="anchor-link" href="posts/word2vec-with-tensorflow/#The-word2vec-model-can-be-trained-with-two-different-word-representations:">¶</a>
</h4>
<ul>
<li>
<b>Continuous Bag-of-Words (CBOW)</b>: predicts target words (e.g. 'mat') from source context words ('the cat sits on the')</li>
<li>
<b>Skip-Gram</b>: predicts source context-words from the target words</li>
</ul>
<h4 id="Skip-Gram-tends-to-do-better-and-this-tutorial-will-implement-a-word2vec-with-skip-grams.">Skip-Gram tends to do better and this tutorial will implement a word2vec with skip-grams.<a class="anchor-link" href="posts/word2vec-with-tensorflow/#Skip-Gram-tends-to-do-better-and-this-tutorial-will-implement-a-word2vec-with-skip-grams.">¶</a>
</h4>
<p>The goal of the model is to train it's embeddings layer in a way that similar by meaning words are close to each other in their N-dimensional vector representation. The model has two layers: the embeddings layer and a linear layer. Because of the last layer is linear, the distance between embedding vectors for words is linearly related to the distance in the meaning of those words. In other words, we are able to do such mathematical operations with the vectors: <b>[king] - [man] + [woman] ~= [queen]</b>
</p>
<p class="more"><a href="posts/word2vec-with-tensorflow/">Read more…</a></p>
</div>
</div>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/embeddings-with-tensorflow/" class="u-url">Embeddings with TensorFlow</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Ivelin Angelov
            </span></p>
            <p class="dateline"><a href="posts/embeddings-with-tensorflow/" rel="bookmark"><time class="published dt-published" datetime="2018-01-19T16:00:40-08:00" title="2018-01-19 16:00">2018-01-19 16:00</time></a></p>
                <p class="commentline">
        
    <a href="posts/embeddings-with-tensorflow/#disqus_thread" data-disqus-identifier="cache/posts/embeddings-with-tensorflow.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Embeddings-in-TensorFlow">Embeddings in TensorFlow<a class="anchor-link" href="posts/embeddings-with-tensorflow/#Embeddings-in-TensorFlow">¶</a>
</h2>
<p>To represent discrete values such as words to a machine learning algorithm, we need to transform every class to a one-hot encoded vector or to an embedding vector.</p>
<p>Using embeddings for a sparse data often results in more efficient representation as compared to the one-hot encoding approach. For example, a typical vocabulary size for NLP problems is usually from 20,000 to 200,000 unique words. It will be very inefficient to represent every word by a vector of thousands of 0s and only one 1.</p>
<p>Embeddings can also be "trained" by an optimizer to have different similarities which could represent semantic similarities between words. For example, a model using trained embeddings could predict a test dataset with words unseen before in the training dataset and still have a logical inference based on similar words before seen when training.</p>
<p>In this post, I'll show and describe use cases of embeddings with Python and TensorFlow.
</p>
<p class="more"><a href="posts/embeddings-with-tensorflow/">Read more…</a></p>
</div>
</div>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/jupyter-blogging/" class="u-url">Jupyter Blogging</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Ivelin Angelov
            </span></p>
            <p class="dateline"><a href="posts/jupyter-blogging/" rel="bookmark"><time class="published dt-published" datetime="2018-01-14T15:01:33-08:00" title="2018-01-14 15:01">2018-01-14 15:01</time></a></p>
                <p class="commentline">
        
    <a href="posts/jupyter-blogging/#disqus_thread" data-disqus-identifier="cache/posts/jupyter-blogging.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Jupyter-Blogging-in-5-minutes.">Jupyter Blogging in 5 minutes.<a class="anchor-link" href="posts/jupyter-blogging/#Jupyter-Blogging-in-5-minutes.">¶</a>
</h2>
<p>Q: What is Jupyter Blogging?<br>
A: Blogging with jupyter notebooks.</p>
<p>Q: Why Jupyter Blogging?<br>
A: As a Data Scientists, I use jupyter to create notebooks with code, equations, visualizations, documentation, etc. "Jupyter Blogging" allows me to share those notebooks with the world without any additional work.</p>
<p>Q: How is this achieved?<br>
A: <a href="http://jupyter.org">Jupyter Notebook</a> + 
<a href="https://pages.github.com">Github Pages</a> + 
<a href="https://getnikola.com">Nikola</a> = Jupyter Blogging.</p>
<p>This tutorial will give you basic instructions for setting up a minimum blog powered by jupyter notebooks. This tutorial is written on a jupyter notebook.
</p>
<p class="more"><a href="posts/jupyter-blogging/">Read more…</a></p>
</div>
</div>
</div>
</div>
</div>
    </div>
    </article>
</div>



        
       <script>var disqus_shortname="post2web-github-io";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script>
</div>
        <!--End of body content-->

        <footer id="footer"><div class="text-center">
<p>
	<span class="fa-stack fa-2x">
	  <a href="rss.xml">
	    <i class="fa fa-circle fa-stack-2x"></i>
	    <i class="fa fa-rss fa-inverse fa-stack-1x"></i>
	  </a>
	</span>
	<span class="fa-stack fa-2x">
	  <a href="https://github.com/post2web">
	    <i class="fa fa-circle fa-stack-2x"></i>
	    <i class="fa fa-github fa-inverse fa-stack-1x"></i>
	  </a>
	</span>
	<span class="fa-stack fa-2x">
	  <a href="https://www.linkedin.com/in/ivelin-angelov">
	    <i class="fa fa-circle fa-stack-2x"></i>
	    <i class="fa fa-linkedin fa-inverse fa-stack-1x"></i>
	  </a>
	</span>
	<span class="fa-stack fa-2x">
	  <a href="mailto:post2web@gmail.com">
	    <i class="fa fa-circle fa-stack-2x"></i>
	    <i class="fa fa-envelope fa-inverse fa-stack-1x"></i>
	  </a>
	</span>
</p>
<p>
  Contents © 2018  <a href="mailto:post2web@gmail.com">Ivelin Angelov</a>
  —
  
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

  —
  Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>
</p>
</div>

            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
