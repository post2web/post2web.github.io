{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings in TensorFlow\n",
    "\n",
    "To represent discrete values such as words to a machine learning algorithm we need to transform every class to a one-hot encoded vector or to an embedding vector. \n",
    "\n",
    "Using embeddings for a sparse data often results in more efficient representation as compared to the one-hot encoding approach. For example, a typical vocabulary size for NLP problems is usually from 20,000 to 200,000 unique words. It will be very inefficient to represent every word by a vector of thousands of 0s were and one 1.\n",
    "\n",
    "Embeddings can also be \"trained\" by an optimizer to have different similarity to each other which could represent semantic similarity between words for example. A model using trained embeddings could, for example, predict a test dataset with words unseen before in the training dataset and still have a logical inference based on similar words before seen when training.\n",
    "\n",
    "In this post, I'll show and describe use cases of embeddings with Python and TensorFlow.\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:48.318071Z",
     "start_time": "2018-01-19T23:50:47.417562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=''\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use words in the text as a use case for demonstrating using embeddings but it is important to point out that embeddings can be used to represent discrete values other than words. Before we feed a text into a machine learning model we need to pre-process it and the first step is often tokenization. I will split some text to create example tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:48.323050Z",
     "start_time": "2018-01-19T23:50:48.319869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the our text: ['my', 'cat', 'is', 'a', 'great', 'cat']\n"
     ]
    }
   ],
   "source": [
    "text = 'My cat is a great cat'\n",
    "tokens = text.lower().split()\n",
    "print('Words in the our text:', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vocabulary out of the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:48.344755Z",
     "start_time": "2018-01-19T23:50:48.324495Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "great    0\n",
       "cat      1\n",
       "my       2\n",
       "a        3\n",
       "is       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(tokens)\n",
    "vocab = pd.Series(range(len(vocab)), index=vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert such text to one-hot vectors we can just use Pandas or any other library in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:48.378233Z",
     "start_time": "2018-01-19T23:50:48.346052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>great</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  cat  great  is  my\n",
       "0  0    0      0   0   1\n",
       "1  0    1      0   0   0\n",
       "2  0    0      0   1   0\n",
       "3  1    0      0   0   0\n",
       "4  0    0      1   0   0\n",
       "5  0    1      0   0   0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding in Python and then sending it to TensorFlow could be very inefficient. It will be better to use the built-in method in TensorFlow ```tf.one_hot for``` that. It expects integer representation for every class and the total number of classes. For our words example, we need to assign a unique integer for every unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:48.394238Z",
     "start_time": "2018-01-19T23:50:48.379497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 4, 3, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = vocab.loc[tokens].values\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot with TensorFlow\n",
    "\n",
    "We are only passing integers instead of potentially huge vectors to TensorFlow and it will internally convert the integers to one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:49.572039Z",
     "start_time": "2018-01-19T23:50:48.395605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# TensorFlow has an operation for one-hot encoding\n",
    "one_hot_inputs = tf.one_hot(inputs, len(vocab))\n",
    "\n",
    "transformed = tf.Session().run(one_hot_inputs, {inputs: word_ids})\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with TensorFlow\n",
    "\n",
    "With embeddings representation, every word will be transformed into a vector of real numbers with a chosen length ```(embedding_size)```.\n",
    "\n",
    "This example is created with ```embedding_size = 3``` in order to easily output the ```embeddings``` vectors. It means that every word is represented by a vector of 3 real numbers. In practice, a common size for word embedding size is 200 or 300.\n",
    "\n",
    "The tensor ```embeddings``` is a two dimensional matrix of type tf.float32 with ```len(vocab)``` rows and ```embedding_size```  columns. The method ```tf.nn.embedding_lookup``` converts our inputs from integers representing words to vectors form the embeddings matrix where every input integer is the index of a row from the ```embeddings```. Every row of the ```embeddings``` matrix is a vector representing a word so every word will be represented as a point in ```embedding_size``` dimensional space. The tensor ```embeddings``` has a random initialization so its content will be different every time and by default, the embeddings will not represent any relationship (as syntactic similarities) between them.\n",
    "\n",
    "The example below transorms our text of six words to a 6x3 array. The second word \"cat\" is the second and the last word in the text so the second resulting vector is the same as the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:49.596759Z",
     "start_time": "2018-01-19T23:50:49.573868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02937758, -0.60863554,  1.1070673 ],\n",
       "       [ 0.5732564 , -0.7388431 ,  0.30292028],\n",
       "       [-0.21285258, -1.8346152 , -1.9110047 ],\n",
       "       [ 0.6378179 ,  1.3454263 , -0.8725002 ],\n",
       "       [-0.89525574, -0.79854083, -0.40036395],\n",
       "       [ 0.5732564 , -0.7388431 ,  0.30292028]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 3\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, [None], name='word_ids')\n",
    "\n",
    "# This is where the embedding vectors live\n",
    "# This will be modified by the optimization unless trainable=False\n",
    "# I choose random normal distribution but you can try other distributions\n",
    "embeddings = tf.random_normal(shape=(len(vocab), embedding_size))\n",
    "\n",
    "# this will return the embedding lookup\n",
    "embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "transformed = sess.run(embedded, {inputs: word_ids})\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the content of the embeddings matrix. It has only five rows because of there are five unique words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:49.604951Z",
     "start_time": "2018-01-19T23:50:49.598569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1316075 , -1.061635  ,  0.7775889 ],\n",
       "       [ 1.5460204 , -0.6848818 ,  0.21591993],\n",
       "       [-1.3386676 ,  1.6173289 ,  0.7851124 ],\n",
       "       [ 0.3835993 ,  0.2665336 , -0.32107848],\n",
       "       [ 0.08313944,  1.1281649 , -1.151932  ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```tf.nn.embedding_lookup``` index to column lookup. If we pass [0, 2] as inputs for the lookup we'll get the first and third row from the embeddings back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:49.620221Z",
     "start_time": "2018-01-19T23:50:49.606348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26338172,  0.06488156, -0.14654125],\n",
       "       [ 0.44013414,  0.22667229,  0.5294355 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(embedded, {inputs: [0, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedings\n",
    "\n",
    "When the tensor ```embeddings``` is created, will be initialised by a random initialization and the distance between words will also have random values. In order to have any similarity between the vectors, we can train them with models like word2vec or use pre-trained vectors.\n",
    "\n",
    "An easy way to get pre-train vectors is with a package called ```chakin```. The project lives [here](https://github.com/chakki-works/chakin). To install it:\n",
    "```\n",
    "pip install chakin\n",
    "```\n",
    "To list the available pre-trained word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:50:49.644411Z",
     "start_time": "2018-01-19T23:50:49.621565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "import chakin\n",
    "\n",
    "chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the Facebooks's fastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:54:02.631002Z",
     "start_time": "2018-01-19T23:50:49.645852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                       | Time: 0:03:12  32.6 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./wiki.en.vec'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chakin.download(number=2, save_dir='.', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It downloads a .vec file in the current directory. The .vec files are text files similar to .csv files. The first column has two numbers for the number of vocabulary and dimensions of the vectors and the reset is token and numbers separated by a space. I just load the file with pd.read_csv but there may be other more efficient ways to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:56:01.064990Z",
     "start_time": "2018-01-19T23:54:02.632934Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.023167</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>-0.105720</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>-0.143160</td>\n",
       "      <td>-0.078954</td>\n",
       "      <td>0.078187</td>\n",
       "      <td>-0.194540</td>\n",
       "      <td>0.022303</td>\n",
       "      <td>0.312070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046595</td>\n",
       "      <td>-0.11558</td>\n",
       "      <td>0.044184</td>\n",
       "      <td>-0.023124</td>\n",
       "      <td>0.025860</td>\n",
       "      <td>-0.116530</td>\n",
       "      <td>0.010936</td>\n",
       "      <td>0.089398</td>\n",
       "      <td>-0.01590</td>\n",
       "      <td>0.148660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.111120</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.177800</td>\n",
       "      <td>0.064508</td>\n",
       "      <td>-0.240370</td>\n",
       "      <td>0.031087</td>\n",
       "      <td>-0.030144</td>\n",
       "      <td>-0.368830</td>\n",
       "      <td>-0.043855</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095332</td>\n",
       "      <td>-0.21914</td>\n",
       "      <td>-0.042760</td>\n",
       "      <td>-0.136850</td>\n",
       "      <td>0.097470</td>\n",
       "      <td>-0.218180</td>\n",
       "      <td>-0.058233</td>\n",
       "      <td>0.063374</td>\n",
       "      <td>-0.12161</td>\n",
       "      <td>0.039339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.065334</td>\n",
       "      <td>-0.093031</td>\n",
       "      <td>-0.017571</td>\n",
       "      <td>0.200070</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>-0.039920</td>\n",
       "      <td>-0.163280</td>\n",
       "      <td>-0.072946</td>\n",
       "      <td>0.089604</td>\n",
       "      <td>0.080907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064944</td>\n",
       "      <td>-0.21673</td>\n",
       "      <td>-0.037683</td>\n",
       "      <td>0.081860</td>\n",
       "      <td>-0.039891</td>\n",
       "      <td>-0.051334</td>\n",
       "      <td>-0.101650</td>\n",
       "      <td>0.166420</td>\n",
       "      <td>-0.13079</td>\n",
       "      <td>0.035397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>0.050258</td>\n",
       "      <td>-0.073228</td>\n",
       "      <td>0.435810</td>\n",
       "      <td>0.174830</td>\n",
       "      <td>-0.185460</td>\n",
       "      <td>-0.399210</td>\n",
       "      <td>-0.507670</td>\n",
       "      <td>-0.506600</td>\n",
       "      <td>-0.155570</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096853</td>\n",
       "      <td>-0.47723</td>\n",
       "      <td>-0.027511</td>\n",
       "      <td>0.259640</td>\n",
       "      <td>-0.010468</td>\n",
       "      <td>-0.298150</td>\n",
       "      <td>-0.236090</td>\n",
       "      <td>0.205250</td>\n",
       "      <td>0.75183</td>\n",
       "      <td>0.097156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.048804</td>\n",
       "      <td>-0.285280</td>\n",
       "      <td>0.018557</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>0.060704</td>\n",
       "      <td>0.085446</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>-0.068373</td>\n",
       "      <td>0.145070</td>\n",
       "      <td>0.178520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169560</td>\n",
       "      <td>-0.33677</td>\n",
       "      <td>-0.060286</td>\n",
       "      <td>0.086097</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>-0.100960</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.039705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7    \\\n",
       "0                                                                            \n",
       ",    -0.023167 -0.004248 -0.105720  0.042783 -0.143160 -0.078954  0.078187   \n",
       ".    -0.111120 -0.001386 -0.177800  0.064508 -0.240370  0.031087 -0.030144   \n",
       "the  -0.065334 -0.093031 -0.017571  0.200070  0.029521 -0.039920 -0.163280   \n",
       "</s>  0.050258 -0.073228  0.435810  0.174830 -0.185460 -0.399210 -0.507670   \n",
       "of    0.048804 -0.285280  0.018557  0.205770  0.060704  0.085446 -0.036267   \n",
       "\n",
       "           8         9         10     ...          291      292       293  \\\n",
       "0                                     ...                                   \n",
       ",    -0.194540  0.022303  0.312070    ...     0.046595 -0.11558  0.044184   \n",
       ".    -0.368830 -0.043855  0.248310    ...     0.095332 -0.21914 -0.042760   \n",
       "the  -0.072946  0.089604  0.080907    ...     0.064944 -0.21673 -0.037683   \n",
       "</s> -0.506600 -0.155570  0.031451    ...    -0.096853 -0.47723 -0.027511   \n",
       "of   -0.068373  0.145070  0.178520    ...     0.169560 -0.33677 -0.060286   \n",
       "\n",
       "           294       295       296       297       298      299       300  \n",
       "0                                                                          \n",
       ",    -0.023124  0.025860 -0.116530  0.010936  0.089398 -0.01590  0.148660  \n",
       ".    -0.136850  0.097470 -0.218180 -0.058233  0.063374 -0.12161  0.039339  \n",
       "the   0.081860 -0.039891 -0.051334 -0.101650  0.166420 -0.13079  0.035397  \n",
       "</s>  0.259640 -0.010468 -0.298150 -0.236090  0.205250  0.75183  0.097156  \n",
       "of    0.086097 -0.065001  0.004833 -0.100960  0.139100 -0.13714 -0.039705  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('wiki.en.vec') as f:\n",
    "    rows, cols = f.readline().strip().split(' ')\n",
    "\n",
    "vectors = pd.read_csv(\n",
    "    'wiki.en.vec', sep=' ', skiprows=1, header=None, index_col=0,\n",
    "    quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# remove one junk column\n",
    "vectors = vectors.dropna(axis=1)\n",
    "assert vectors.shape == (int(rows), int(cols))\n",
    "vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word in this model is represented by a vector of 300 number. Here is the vector for the word \"car\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:56:01.515459Z",
     "start_time": "2018-01-19T23:56:01.066574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>-0.092271</td>\n",
       "      <td>-0.14855</td>\n",
       "      <td>-0.14696</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.40305</td>\n",
       "      <td>-0.31004</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>-0.42087</td>\n",
       "      <td>-0.22948</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096352</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.31818</td>\n",
       "      <td>-0.18818</td>\n",
       "      <td>0.14998</td>\n",
       "      <td>-0.18162</td>\n",
       "      <td>-0.35564</td>\n",
       "      <td>0.28245</td>\n",
       "      <td>-0.18557</td>\n",
       "      <td>-0.060884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1        2        3      4        5        6       7        8    \\\n",
       "0                                                                           \n",
       "car -0.092271 -0.14855 -0.14696  0.013 -0.40305 -0.31004  0.1022 -0.42087   \n",
       "\n",
       "         9        10     ...          291       292      293      294  \\\n",
       "0                        ...                                            \n",
       "car -0.22948  0.12853    ...     0.096352  0.031328  0.31818 -0.18818   \n",
       "\n",
       "         295      296      297      298      299       300  \n",
       "0                                                           \n",
       "car  0.14998 -0.18162 -0.35564  0.28245 -0.18557 -0.060884  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.loc[['car']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate similarity, we can use cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:56:02.359473Z",
     "start_time": "2018-01-19T23:56:01.516897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:\n",
      "   bus to car = 0.46000568082170107\n",
      "   bus to dog = 0.17041678778979216\n",
      "   dog to cat = 0.6380517245741392\n",
      "   cat to car = 0.15984959272698984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Similarity:')\n",
    "print('   bus to car =', cosine_similarity(vectors.loc[['bus']], vectors.loc[['car']])[0][0])\n",
    "print('   bus to dog =', cosine_similarity(vectors.loc[['bus']], vectors.loc[['cat']])[0][0])\n",
    "print('   dog to cat =', cosine_similarity(vectors.loc[['dog']], vectors.loc[['cat']])[0][0])\n",
    "print('   cat to car =', cosine_similarity(vectors.loc[['dog']], vectors.loc[['bus']])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this vectors for our vocabulary, we need to put them in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:56:02.597936Z",
     "start_time": "2018-01-19T23:56:02.361185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-0.267620</td>\n",
       "      <td>-0.081004</td>\n",
       "      <td>-0.21283</td>\n",
       "      <td>0.380140</td>\n",
       "      <td>-0.156980</td>\n",
       "      <td>-0.167050</td>\n",
       "      <td>0.370720</td>\n",
       "      <td>0.108050</td>\n",
       "      <td>0.293960</td>\n",
       "      <td>0.159460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115460</td>\n",
       "      <td>-0.10987</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>-0.071547</td>\n",
       "      <td>-0.253600</td>\n",
       "      <td>-0.101550</td>\n",
       "      <td>-0.013372</td>\n",
       "      <td>0.105520</td>\n",
       "      <td>0.037726</td>\n",
       "      <td>0.253640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>-0.138190</td>\n",
       "      <td>0.140290</td>\n",
       "      <td>-0.32621</td>\n",
       "      <td>0.116240</td>\n",
       "      <td>-0.198060</td>\n",
       "      <td>0.455260</td>\n",
       "      <td>0.212820</td>\n",
       "      <td>-0.512560</td>\n",
       "      <td>0.033657</td>\n",
       "      <td>0.154290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151850</td>\n",
       "      <td>-0.32703</td>\n",
       "      <td>0.102690</td>\n",
       "      <td>-0.053309</td>\n",
       "      <td>-0.068975</td>\n",
       "      <td>-0.006616</td>\n",
       "      <td>-0.066738</td>\n",
       "      <td>0.273190</td>\n",
       "      <td>0.520030</td>\n",
       "      <td>-0.008721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>-0.126860</td>\n",
       "      <td>0.152880</td>\n",
       "      <td>0.14903</td>\n",
       "      <td>0.039269</td>\n",
       "      <td>-0.130520</td>\n",
       "      <td>-0.038069</td>\n",
       "      <td>-0.162300</td>\n",
       "      <td>-0.002766</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.142020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062873</td>\n",
       "      <td>0.14942</td>\n",
       "      <td>-0.146160</td>\n",
       "      <td>-0.210860</td>\n",
       "      <td>0.321350</td>\n",
       "      <td>-0.037258</td>\n",
       "      <td>-0.060301</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>-0.123030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.115590</td>\n",
       "      <td>0.301920</td>\n",
       "      <td>-0.11465</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-0.032187</td>\n",
       "      <td>-0.107550</td>\n",
       "      <td>0.060674</td>\n",
       "      <td>-0.104770</td>\n",
       "      <td>0.174880</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.18694</td>\n",
       "      <td>-0.065594</td>\n",
       "      <td>-0.202230</td>\n",
       "      <td>-0.122180</td>\n",
       "      <td>-0.297980</td>\n",
       "      <td>0.034272</td>\n",
       "      <td>0.110480</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>0.041164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.035927</td>\n",
       "      <td>0.145170</td>\n",
       "      <td>0.11926</td>\n",
       "      <td>0.078836</td>\n",
       "      <td>-0.047748</td>\n",
       "      <td>0.100960</td>\n",
       "      <td>0.090815</td>\n",
       "      <td>-0.221760</td>\n",
       "      <td>-0.095085</td>\n",
       "      <td>-0.022610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040324</td>\n",
       "      <td>-0.27410</td>\n",
       "      <td>-0.116330</td>\n",
       "      <td>-0.089418</td>\n",
       "      <td>-0.072754</td>\n",
       "      <td>-0.260430</td>\n",
       "      <td>0.084246</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>-0.035512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2        3         4         5         6         7    \\\n",
       "0                                                                            \n",
       "great -0.267620 -0.081004 -0.21283  0.380140 -0.156980 -0.167050  0.370720   \n",
       "cat   -0.138190  0.140290 -0.32621  0.116240 -0.198060  0.455260  0.212820   \n",
       "my    -0.126860  0.152880  0.14903  0.039269 -0.130520 -0.038069 -0.162300   \n",
       "a      0.115590  0.301920 -0.11465  0.010010 -0.032187 -0.107550  0.060674   \n",
       "is     0.035927  0.145170  0.11926  0.078836 -0.047748  0.100960  0.090815   \n",
       "\n",
       "            8         9         10     ...          291      292       293  \\\n",
       "0                                      ...                                   \n",
       "great  0.108050  0.293960  0.159460    ...    -0.115460 -0.10987  0.285300   \n",
       "cat   -0.512560  0.033657  0.154290    ...     0.151850 -0.32703  0.102690   \n",
       "my    -0.002766  0.121190  0.142020    ...     0.062873  0.14942 -0.146160   \n",
       "a     -0.104770  0.174880  0.008112    ...    -0.020257 -0.18694 -0.065594   \n",
       "is    -0.221760 -0.095085 -0.022610    ...     0.040324 -0.27410 -0.116330   \n",
       "\n",
       "            294       295       296       297       298       299       300  \n",
       "0                                                                            \n",
       "great -0.071547 -0.253600 -0.101550 -0.013372  0.105520  0.037726  0.253640  \n",
       "cat   -0.053309 -0.068975 -0.006616 -0.066738  0.273190  0.520030 -0.008721  \n",
       "my    -0.210860  0.321350 -0.037258 -0.060301  0.419110  0.032854 -0.123030  \n",
       "a     -0.202230 -0.122180 -0.297980  0.034272  0.110480  0.130740  0.041164  \n",
       "is    -0.089418 -0.072754 -0.260430  0.084246 -0.001608  0.170800 -0.035512  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = vectors.loc[vocab.index, :]\n",
    "pretrained_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Embeddings with TensorFlow\n",
    "\n",
    "Instead of random values I can now initialize the ```embeddings``` with ```pretrained_embeddings```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:56:02.623839Z",
     "start_time": "2018-01-19T23:56:02.599369Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12686  ,  0.15288  ,  0.14903  , ...,  0.41911  ,  0.032854 ,\n",
       "        -0.12303  ],\n",
       "       [-0.13819  ,  0.14029  , -0.32621  , ...,  0.27319  ,  0.52003  ,\n",
       "        -0.0087214],\n",
       "       [ 0.035927 ,  0.14517  ,  0.11926  , ..., -0.0016082,  0.1708   ,\n",
       "        -0.035512 ],\n",
       "       [ 0.11559  ,  0.30192  , -0.11465  , ...,  0.11048  ,  0.13074  ,\n",
       "         0.041164 ],\n",
       "       [-0.26762  , -0.081004 , -0.21283  , ...,  0.10552  ,  0.037726 ,\n",
       "         0.25364  ],\n",
       "       [-0.13819  ,  0.14029  , -0.32621  , ...,  0.27319  ,  0.52003  ,\n",
       "        -0.0087214]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "embeddings = tf.Variable(pretrained_embeddings.values)\n",
    "\n",
    "embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "transformed = sess.run(embedded, {inputs: word_ids})\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several models for \"similarity training\" of embeddings. The most popular are:\n",
    "\n",
    "- [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText](https://github.com/facebookresearch/fastText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nikola": {
   "category": "",
   "date": "2018-01-19 16:00:40 UTC-08:00",
   "description": "",
   "link": "",
   "slug": "embeddings-with-tensorflow",
   "tags": "",
   "title": "Embeddings with TensorFlow",
   "type": "text"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
