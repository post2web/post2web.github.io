{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Skip-Gram and TensorFlow\n",
    "\n",
    "This is a tutorial and a basic example for getting started with word2vec model by [Mikolov et al](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It is used for learning vector representations of words, called \"Words Embeddings\". For more information about Embeddings read my previous post. \n",
    "\n",
    "### The word2vec model can be trained with two different word representation:\n",
    "- <b>Continuous Bag-of-Words (CBOW)</b>: predicts target words (e.g. 'mat') from source context words ('the cat sits on the')\n",
    "- <b>Skip-Gram</b>: predicts source context-words from the target words\n",
    "\n",
    "### Skip-Gram tends to do better and this tutorial will implement a word2vec with skip-grams.\n",
    "The goal of the model is to train it embeddings layer in a way that similar by meaning words are close to each other in their N-dimensional vector representation. The model has two layers: the embeddings layer and a linear layer. Because of the last layer is linear, the distance between embedding vectors for words is linearly related to the distance in the meaning of those words. In other words, we are able to do such mathematical operations with the vectors:\n",
    "\n",
    "#### [king] - [man] + [woman] ~= [queen]\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:21:33.337119Z",
     "start_time": "2018-01-20T06:21:32.146930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "To train a word2vec model we need a large text corpus. This example uses text from the \"20 newsgroups dataset\". The dataset contains 11314 messages form a message board with corresponding labels for its topic. We just merge all messages together and ignore the labels. In practice, it's better to use a larger corpus and to have a domain-specific text. Lowering the case of the text is optional and will small corpus will be better ot have it lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:21:33.687569Z",
     "start_time": "2018-01-20T06:21:33.338822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'umd.edu\\norganization: university of maryland, college park\\nlines: 15\\n\\n i was wondering if anyone out there could enlighten me on this car i saw\\nthe other day. it was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. it was called a bric'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "text = ' '.join(data.data).lower()\n",
    "text[100:350]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T01:46:41.573105Z",
     "start_time": "2018-01-20T01:46:41.487327Z"
    }
   },
   "source": [
    "# Sentence Tokenize\n",
    "\n",
    "The skip grams will work better if they are created from sentenced text. ```nltk.sent_tokenize``` will break a string to a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:21:42.273360Z",
     "start_time": "2018-01-20T06:21:33.689146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173268"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_text = nltk.sent_tokenize(text)\n",
    "len(sentences_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenize\n",
    "\n",
    "Next, break all sentences to tokens (words) with ```nltk.word_tokenize```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:22:19.027607Z",
     "start_time": "2018-01-20T06:21:42.274908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [nltk.word_tokenize(s) for s in sentences_text]\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary (unique words)\n",
    "\n",
    "In this example, we filter words who are used less than 5 times in the text, stop words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:22:20.011301Z",
     "start_time": "2018-01-20T06:22:19.029119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary has: 34016 words\n"
     ]
    }
   ],
   "source": [
    "from collections import  Counter\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "min_count = 5\n",
    "puncs = set(punctuation)\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "flat_words = []\n",
    "for sentence in sentences:\n",
    "    flat_words += sentence\n",
    "    \n",
    "counts = Counter(list(flat_words))\n",
    "counts = pd.DataFrame(counts.most_common())\n",
    "counts.columns = ['word', 'count']\n",
    "\n",
    "counts = counts[counts['count'] >= min_count]\n",
    "counts = counts[~counts['word'].isin(puncs)]\n",
    "counts = counts[~counts['word'].isin(stops)]\n",
    "\n",
    "\n",
    "vocab = pd.Series(range(len(counts)), index=counts['word']).sort_index()\n",
    "\n",
    "print('The vocabulary has:', len(vocab), 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter tokens not in vocabulary\n",
    "\n",
    "Some words were excluded from the vocabulary because they are very rare or too common to present value. We have to remove them from our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:22:23.291119Z",
     "start_time": "2018-01-20T06:22:20.013049Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = [word for word in sentence if word in vocab.index]\n",
    "    if len(sentence):\n",
    "        filtered_sentences.append(sentence)\n",
    "sentences = filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the words to integer indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:25.362175Z",
     "start_time": "2018-01-20T06:22:23.292810Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentences[i] = [vocab.loc[word] for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Skip-Gram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.625844Z",
     "start_time": "2018-01-20T06:23:25.363656Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5816</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5816</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5816</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5816</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x    y\n",
       "0  5816    4\n",
       "1  5816  122\n",
       "2  5816    6\n",
       "3  5816  159\n",
       "4     4  122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "data = []\n",
    "for sentance in sentences:\n",
    "    data += skipgrams(sentance, 2, window_size)\n",
    "\n",
    "data = pd.DataFrame(data, columns=['x', 'y'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.630994Z",
     "start_time": "2018-01-20T06:23:37.627190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14098409 Validation size: 5000\n"
     ]
    }
   ],
   "source": [
    "validation_size = 5000\n",
    "\n",
    "data_valid = data.iloc[-validation_size:]\n",
    "data_train = data.iloc[:-validation_size]\n",
    "print('Train size:', len(data_train), 'Validation size:', len(data_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.642075Z",
     "start_time": "2018-01-20T06:23:37.632630Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = .01\n",
    "embed_size = 300\n",
    "batch_size = 64\n",
    "steps = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.669574Z",
     "start_time": "2018-01-20T06:23:37.643391Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None])\n",
    "targets = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Layer\n",
    "\n",
    "This is the embeddings layer. Its a len(vocab) by embed_size matrix, initialized with random uniform distribution. The optimizer will change the similarity between it's rows to be higher on similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.678914Z",
     "start_time": "2018-01-20T06:23:37.671013Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embeddings, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear layer\n",
    "We use a linear layer with ```activation=None```. We don't need this layer after the training. Think of it as part of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.695905Z",
     "start_time": "2018-01-20T06:23:37.680256Z"
    }
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(embed, len(vocab), activation=None,\n",
    "    kernel_initializer=tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Optimization\n",
    "\n",
    "There is a more optimized, noise-contrastive loss function for traning word embeddings: ```tf.nn.nce_loss```. I use ```tf.nn.softmax_cross_entropy_with_logits``` for simplicity. For more information about the nce_loss look at the [TensorFlow word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:37.771509Z",
     "start_time": "2018-01-20T06:23:37.697241Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = tf.one_hot(targets, len(vocab))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:23:39.379210Z",
     "start_time": "2018-01-20T06:23:37.773161Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T08:13:27.564296Z",
     "start_time": "2018-01-20T06:23:39.382017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000 TLoss: 2.0164194 VLoss: 0.5388802 Similarity: 0.023 Seconds 66.5\n",
      "Step: 20000 TLoss: 0.12752666 VLoss: 0.11306174 Similarity: 0.023 Seconds 65.2\n",
      "Step: 30000 TLoss: 0.028313937 VLoss: 0.11745877 Similarity: 0.023 Seconds 65.5\n",
      "Step: 40000 TLoss: 0.010771247 VLoss: 0.012606331 Similarity: 0.024 Seconds 65.8\n",
      "Step: 50000 TLoss: 0.0013680928 VLoss: 0.012557062 Similarity: 0.024 Seconds 65.9\n",
      "Step: 60000 TLoss: 0.0041248337 VLoss: 7.6293943e-10 Similarity: 0.025 Seconds 65.8\n",
      "Step: 70000 TLoss: 0.0014730034 VLoss: 1.0251998e-09 Similarity: 0.026 Seconds 65.9\n",
      "Step: 80000 TLoss: 0.00054400414 VLoss: 6.67572e-10 Similarity: 0.027 Seconds 65.9\n",
      "Step: 90000 TLoss: 0.0005858248 VLoss: 1.3113021e-09 Similarity: 0.027 Seconds 65.9\n",
      "Step: 100000 TLoss: 0.0009175814 VLoss: 8.1062307e-10 Similarity: 0.028 Seconds 65.9\n",
      "Step: 110000 TLoss: 0.00022360244 VLoss: 9.059905e-10 Similarity: 0.029 Seconds 66.9\n",
      "Step: 120000 TLoss: 6.2553576e-05 VLoss: 6.4373007e-10 Similarity: 0.030 Seconds 65.8\n",
      "Step: 130000 TLoss: 0.00019555185 VLoss: 2.4318687e-09 Similarity: 0.030 Seconds 65.8\n",
      "Step: 140000 TLoss: 5.0704068e-05 VLoss: 9.775161e-10 Similarity: 0.031 Seconds 65.8\n",
      "Step: 150000 TLoss: 0.00014003972 VLoss: 1.0251997e-09 Similarity: 0.032 Seconds 65.9\n",
      "Step: 160000 TLoss: 7.326659e-05 VLoss: 1.2636184e-09 Similarity: 0.033 Seconds 65.9\n",
      "Step: 170000 TLoss: 0.00014974378 VLoss: 9.775161e-10 Similarity: 0.033 Seconds 65.8\n",
      "Step: 180000 TLoss: 1.296028e-09 VLoss: 1.7404554e-09 Similarity: 0.034 Seconds 65.8\n",
      "Step: 190000 TLoss: 9.8594224e-05 VLoss: 8.583068e-10 Similarity: 0.035 Seconds 65.8\n",
      "Step: 200000 TLoss: 5.5742334e-05 VLoss: 1.3113021e-09 Similarity: 0.036 Seconds 65.8\n",
      "Step: 210000 TLoss: 1.2936069e-09 VLoss: 9.536743e-10 Similarity: 0.037 Seconds 66.9\n",
      "Step: 220000 TLoss: 1.3055278e-09 VLoss: 1.4066694e-09 Similarity: 0.037 Seconds 65.8\n",
      "Step: 230000 TLoss: 0.0001788908 VLoss: 1.0490416e-09 Similarity: 0.038 Seconds 65.8\n",
      "Step: 240000 TLoss: 5.5293618e-05 VLoss: 2.4795528e-09 Similarity: 0.039 Seconds 65.7\n",
      "Step: 250000 TLoss: 1.3612207e-09 VLoss: 1.5735624e-09 Similarity: 0.040 Seconds 65.8\n",
      "Step: 260000 TLoss: 1.3375653e-09 VLoss: 1.0728836e-09 Similarity: 0.041 Seconds 65.8\n",
      "Step: 270000 TLoss: 1.3418494e-09 VLoss: 1.5258788e-09 Similarity: 0.042 Seconds 65.8\n",
      "Step: 280000 TLoss: 1.3548879e-09 VLoss: 1.001358e-09 Similarity: 0.043 Seconds 65.7\n",
      "Step: 290000 TLoss: 1.3586132e-09 VLoss: 1.2159346e-09 Similarity: 0.045 Seconds 65.8\n",
      "Step: 300000 TLoss: 0.0001109721 VLoss: 9.298324e-10 Similarity: 0.046 Seconds 65.8\n",
      "Step: 310000 TLoss: 1.3770534e-09 VLoss: 7.867812e-10 Similarity: 0.047 Seconds 66.9\n",
      "Step: 320000 TLoss: 1.3697891e-09 VLoss: 8.1062307e-10 Similarity: 0.048 Seconds 65.7\n",
      "Step: 330000 TLoss: 1.3677401e-09 VLoss: 8.8214863e-10 Similarity: 0.050 Seconds 65.8\n",
      "Step: 340000 TLoss: 1.3770534e-09 VLoss: 1.0490416e-09 Similarity: 0.051 Seconds 65.7\n",
      "Step: 350000 TLoss: 1.3750044e-09 VLoss: 9.775161e-10 Similarity: 0.052 Seconds 65.8\n",
      "Step: 360000 TLoss: 1.3889743e-09 VLoss: 1.2159347e-09 Similarity: 0.054 Seconds 65.8\n",
      "Step: 370000 TLoss: 1.3785435e-09 VLoss: 9.298324e-10 Similarity: 0.055 Seconds 65.8\n",
      "Step: 380000 TLoss: 1.3930721e-09 VLoss: 9.536743e-10 Similarity: 0.057 Seconds 65.8\n",
      "Step: 390000 TLoss: 1.3809649e-09 VLoss: 1.2397765e-09 Similarity: 0.059 Seconds 65.8\n",
      "Step: 400000 TLoss: 1.3953073e-09 VLoss: 1.1205672e-09 Similarity: 0.060 Seconds 65.8\n",
      "Step: 410000 TLoss: 1.3913958e-09 VLoss: 1.3589857e-09 Similarity: 0.062 Seconds 67.0\n",
      "Step: 420000 TLoss: 1.4010815e-09 VLoss: 1.4066694e-09 Similarity: 0.064 Seconds 65.7\n",
      "Step: 430000 TLoss: 1.400709e-09 VLoss: 1.2874601e-09 Similarity: 0.066 Seconds 65.8\n",
      "Step: 440000 TLoss: 1.4089047e-09 VLoss: 1.5258788e-09 Similarity: 0.068 Seconds 65.8\n",
      "Step: 450000 TLoss: 1.407787e-09 VLoss: 1.9550321e-09 Similarity: 0.070 Seconds 65.8\n",
      "Step: 460000 TLoss: 1.4074145e-09 VLoss: 1.2159347e-09 Similarity: 0.072 Seconds 65.8\n",
      "Step: 470000 TLoss: 1.4100222e-09 VLoss: 1.2397764e-09 Similarity: 0.074 Seconds 65.9\n",
      "Step: 480000 TLoss: 1.4215706e-09 VLoss: 9.536743e-10 Similarity: 0.076 Seconds 65.8\n",
      "Step: 490000 TLoss: 1.4159827e-09 VLoss: 9.298324e-10 Similarity: 0.079 Seconds 65.8\n",
      "Step: 500000 TLoss: 1.416169e-09 VLoss: 9.298324e-10 Similarity: 0.081 Seconds 65.8\n",
      "Step: 510000 TLoss: 1.4213843e-09 VLoss: 9.298324e-10 Similarity: 0.083 Seconds 66.9\n",
      "Step: 520000 TLoss: 1.4251096e-09 VLoss: 1.0967254e-09 Similarity: 0.086 Seconds 65.8\n",
      "Step: 530000 TLoss: 1.4256684e-09 VLoss: 1.0251998e-09 Similarity: 0.089 Seconds 65.8\n",
      "Step: 540000 TLoss: 1.4100222e-09 VLoss: 9.536743e-10 Similarity: 0.092 Seconds 65.8\n",
      "Step: 550000 TLoss: 6.02954e-05 VLoss: 1.3828276e-09 Similarity: 0.095 Seconds 65.8\n",
      "Step: 560000 TLoss: 1.4210118e-09 VLoss: 9.298324e-10 Similarity: 0.097 Seconds 65.8\n",
      "Step: 570000 TLoss: 1.4185904e-09 VLoss: 1.0967254e-09 Similarity: 0.100 Seconds 65.8\n",
      "Step: 580000 TLoss: 1.4187767e-09 VLoss: 1.7404554e-09 Similarity: 0.104 Seconds 65.9\n",
      "Step: 590000 TLoss: 1.4262272e-09 VLoss: 1.1444091e-09 Similarity: 0.107 Seconds 65.8\n",
      "Step: 600000 TLoss: 1.4228745e-09 VLoss: 1.0728836e-09 Similarity: 0.110 Seconds 65.7\n",
      "Step: 610000 TLoss: 1.4236196e-09 VLoss: 1.0967254e-09 Similarity: 0.113 Seconds 66.9\n",
      "Step: 620000 TLoss: 1.4243645e-09 VLoss: 1.1920928e-09 Similarity: 0.117 Seconds 65.8\n",
      "Step: 630000 TLoss: 1.4230607e-09 VLoss: 1.5258789e-09 Similarity: 0.120 Seconds 65.9\n",
      "Step: 640000 TLoss: 1.4180316e-09 VLoss: 1.0490416e-09 Similarity: 0.124 Seconds 65.8\n",
      "Step: 650000 TLoss: 1.4292074e-09 VLoss: 1.2159346e-09 Similarity: 0.127 Seconds 65.7\n",
      "Step: 660000 TLoss: 1.4111398e-09 VLoss: 1.2874602e-09 Similarity: 0.131 Seconds 65.9\n",
      "Step: 670000 TLoss: 1.4150513e-09 VLoss: 1.0251998e-09 Similarity: 0.135 Seconds 65.8\n",
      "Step: 680000 TLoss: 1.4187767e-09 VLoss: 1.2397764e-09 Similarity: 0.138 Seconds 65.8\n",
      "Step: 690000 TLoss: 1.4150513e-09 VLoss: 1.5735624e-09 Similarity: 0.142 Seconds 65.8\n",
      "Step: 700000 TLoss: 1.4124436e-09 VLoss: 1.2397764e-09 Similarity: 0.146 Seconds 65.8\n",
      "Step: 710000 TLoss: 1.4159827e-09 VLoss: 6.437301e-10 Similarity: 0.189 Seconds 66.9\n",
      "Step: 720000 TLoss: 1.5705822e-09 VLoss: 1.3589857e-09 Similarity: 0.246 Seconds 65.7\n",
      "Step: 730000 TLoss: 1.493655e-09 VLoss: 1.9073485e-09 Similarity: 0.250 Seconds 65.8\n",
      "Step: 740000 TLoss: 1.474656e-09 VLoss: 1.3351439e-09 Similarity: 0.254 Seconds 65.8\n",
      "Step: 750000 TLoss: 1.4690681e-09 VLoss: 1.9073485e-09 Similarity: 0.258 Seconds 65.7\n",
      "Step: 760000 TLoss: 1.4789401e-09 VLoss: 1.1205672e-09 Similarity: 0.262 Seconds 65.7\n",
      "Step: 770000 TLoss: 1.462735e-09 VLoss: 1.4066696e-09 Similarity: 0.266 Seconds 65.7\n",
      "Step: 780000 TLoss: 0.00051325257 VLoss: 4.7683715e-11 Similarity: 0.209 Seconds 65.8\n",
      "Step: 790000 TLoss: 2.8088684e-10 VLoss: 1.9073486e-10 Similarity: 0.212 Seconds 65.7\n",
      "Step: 800000 TLoss: 3.341585e-10 VLoss: 2.3841856e-10 Similarity: 0.214 Seconds 65.8\n",
      "Step: 810000 TLoss: 3.8016584e-10 VLoss: 6.67572e-10 Similarity: 0.216 Seconds 66.8\n",
      "Step: 820000 TLoss: 4.0736045e-10 VLoss: 5.245208e-10 Similarity: 0.217 Seconds 65.7\n",
      "Step: 830000 TLoss: 4.308298e-10 VLoss: 3.814697e-10 Similarity: 0.219 Seconds 65.7\n",
      "Step: 840000 TLoss: 4.3716278e-10 VLoss: 3.5762784e-10 Similarity: 0.220 Seconds 65.7\n",
      "Step: 850000 TLoss: 4.4349577e-10 VLoss: 7.390975e-10 Similarity: 0.221 Seconds 65.7\n",
      "Step: 860000 TLoss: 4.5541665e-10 VLoss: 3.8146972e-10 Similarity: 0.223 Seconds 65.8\n",
      "Step: 870000 TLoss: 4.706904e-10 VLoss: 4.291534e-10 Similarity: 0.224 Seconds 65.7\n",
      "Step: 880000 TLoss: 4.716217e-10 VLoss: 4.768371e-10 Similarity: 0.225 Seconds 65.7\n",
      "Step: 890000 TLoss: 4.7758214e-10 VLoss: 7.3909756e-10 Similarity: 0.227 Seconds 65.8\n",
      "Step: 900000 TLoss: 4.872679e-10 VLoss: 5.245208e-10 Similarity: 0.228 Seconds 65.7\n",
      "Step: 910000 TLoss: 4.8801296e-10 VLoss: 4.5299525e-10 Similarity: 0.229 Seconds 66.8\n",
      "Step: 920000 TLoss: 5.003064e-10 VLoss: 7.629394e-10 Similarity: 0.230 Seconds 65.7\n",
      "Step: 930000 TLoss: 5.122273e-10 VLoss: 5.00679e-10 Similarity: 0.232 Seconds 65.7\n",
      "Step: 940000 TLoss: 5.0999216e-10 VLoss: 3.33786e-10 Similarity: 0.233 Seconds 65.7\n",
      "Step: 950000 TLoss: 5.222856e-10 VLoss: 2.8610228e-10 Similarity: 0.234 Seconds 65.8\n",
      "Step: 960000 TLoss: 5.30295e-10 VLoss: 6.1988825e-10 Similarity: 0.235 Seconds 65.8\n",
      "Step: 970000 TLoss: 5.427747e-10 VLoss: 6.1988825e-10 Similarity: 0.236 Seconds 65.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 980000 TLoss: 5.3048127e-10 VLoss: 5.245208e-10 Similarity: 0.237 Seconds 65.8\n",
      "Step: 990000 TLoss: 5.479901e-10 VLoss: 5.483627e-10 Similarity: 0.238 Seconds 65.7\n",
      "Step: 1000000 TLoss: 5.507841e-10 VLoss: 4.5299525e-10 Similarity: 0.239 Seconds 65.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_batches(x, y, batch_size, n=None):\n",
    "    if n:\n",
    "        # cheap way to add some randomization\n",
    "        rand_start = np.random.randint(0, len(x) - batch_size * n)\n",
    "        x = x[rand_start:]\n",
    "        y = y[rand_start:]\n",
    "\n",
    "    for start in range(len(x))[::batch_size][:n]:\n",
    "        end = start + batch_size\n",
    "        yield x[start:end], y[start:end]\n",
    "\n",
    "step = 0\n",
    "while step < steps:\n",
    "    start = time.time()\n",
    "    \n",
    "    # shuffle train data once in while\n",
    "    if step % 100000 == 0:\n",
    "        data_train = data_train.sample(frac=1.)\n",
    "    \n",
    "    # train part\n",
    "    train_loss = []\n",
    "    for x, y in get_batches(\n",
    "        data_train['x'].values, data_train['x'].values, batch_size, n=10000):\n",
    "        step += 1\n",
    "        _, batch_loss = sess.run([train_op, loss], {inputs: x, targets: y})\n",
    "        train_loss.append(batch_loss)\n",
    "\n",
    "    # validation prat (one batch of \"validation_size\")\n",
    "    feed_dict = {inputs: data_valid['x'].values, targets: data_valid['x'].values}\n",
    "    valid_loss, x_vectors = sess.run([loss, embed], feed_dict)\n",
    "    y_vectors = sess.run(embed, {inputs: data_valid['x'].values})\n",
    "\n",
    "    # outputs\n",
    "    print('Step:', step, 'TLoss:', np.mean(train_loss), 'VLoss:', np.mean(valid_loss),\n",
    "          'Similarity: %.3f' % cosine_similarity(x_vectors, y_vectors).mean(),\n",
    "          'Seconds %.1f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have trained embeddings! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:35:34.410265Z",
     "start_time": "2018-01-20T16:35:34.384822Z"
    }
   },
   "outputs": [],
   "source": [
    "vectors = sess.run(embeddings)\n",
    "vectors = pd.DataFrame(vectors, index=vocab.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:36:20.251443Z",
     "start_time": "2018-01-20T16:36:20.227812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:\n",
      "   computer to mouse = 0.05870525\n",
      "   cat to mouse = 0.052366085\n",
      "   dog to mouse = -0.009641118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Similarity:')\n",
    "print('   computer to mouse =', cosine_similarity(vectors.loc[['computer']], vectors.loc[['mouse']])[0][0])\n",
    "print('   cat to mouse =', cosine_similarity(vectors.loc[['cat']], vectors.loc[['mouse']])[0][0])\n",
    "print('   dog to mouse =', cosine_similarity(vectors.loc[['dog']], vectors.loc[['mouse']])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://www.tensorflow.org/tutorials/word2vec\n",
    "- http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nikola": {
   "category": "",
   "date": "2018-01-20 13:00:07 UTC-08:00",
   "description": "",
   "link": "",
   "slug": "word2vec-with-tensorflow",
   "tags": "",
   "title": "Word2Vec with TensorFlow",
   "type": "text"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
