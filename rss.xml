<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Evo's TL; DR Data Science Blog</title><link>https://post2web.github.io/</link><description>This is my data science blog.</description><atom:link rel="self" type="application/rss+xml" href="https://post2web.github.io/rss.xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:post2web@gmail.com"&gt;Ivelin Angelov&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
</copyright><lastBuildDate>Sat, 20 Jan 2018 00:03:41 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Embeddings with TensorFlow</title><link>https://post2web.github.io/posts/embeddings-with-tensorflow/</link><dc:creator>Ivelin Angelov</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Embeddings-in-TensorFlow"&gt;Embeddings in TensorFlow&lt;a class="anchor-link" href="https://post2web.github.io/posts/embeddings-with-tensorflow/#Embeddings-in-TensorFlow"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To represent discrete values such as words to a machine learning algorithm we need to transform every class to a one-hot encoded vector or to an embedding vector.&lt;/p&gt;
&lt;p&gt;Using embeddings for a sparse data often results in more efficient representation as compared to the one-hot encoding approach. For example, a typical vocabulary size for NLP problems is usually from 20,000 to 200,000 unique words. It will be very inefficient to represent every word by a vector of thousands of 0s were and one 1.&lt;/p&gt;
&lt;p&gt;Embeddings can also be "trained" by an optimizer to have different similarity to each other which could represent semantic similarity between words for example. A model using trained embeddings could, for example, predict a test dataset with words unseen before in the training dataset and still have a logical inference based on similar words before seen when training.&lt;/p&gt;
&lt;p&gt;In this post, I'll show and describe use cases of embeddings with Python and TensorFlow.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://post2web.github.io/posts/embeddings-with-tensorflow/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://post2web.github.io/posts/embeddings-with-tensorflow/</guid><pubDate>Sat, 20 Jan 2018 00:00:40 GMT</pubDate></item><item><title>Jupyter Blogging</title><link>https://post2web.github.io/posts/jupyter-blogging/</link><dc:creator>Ivelin Angelov</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Jupyter-Blogging-in-5-minutes."&gt;Jupyter Blogging in 5 minutes.&lt;a class="anchor-link" href="https://post2web.github.io/posts/jupyter-blogging/#Jupyter-Blogging-in-5-minutes."&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Q: What is Jupyter Blogging?&lt;br&gt;
A: Blogging with jupyter notebooks.&lt;/p&gt;
&lt;p&gt;Q: Why Jupyter Blogging?&lt;br&gt;
A: As a Data Scientists, I use jupyter to create notebooks with code, equations, visualizations, documentation, etc. "Jupyter Blogging" allows me to share those notebooks with the world without any additional work.&lt;/p&gt;
&lt;p&gt;Q: How is this achieved?&lt;br&gt;
A: &lt;a href="http://jupyter.org"&gt;Jupyter Notebook&lt;/a&gt; + 
&lt;a href="https://pages.github.com"&gt;Github Pages&lt;/a&gt; + 
&lt;a href="https://getnikola.com"&gt;Nikola&lt;/a&gt; = Jupyter Blogging.&lt;/p&gt;
&lt;p&gt;This tutorial will give you basic instructions for setting up a minimum blog powered by jupyter notebooks. This tutorial is written on a jupyter notebook.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://post2web.github.io/posts/jupyter-blogging/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://post2web.github.io/posts/jupyter-blogging/</guid><pubDate>Sun, 14 Jan 2018 23:01:33 GMT</pubDate></item></channel></rss>