<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Ivelin's TL;DR Data Science Blog</title><link>https://post2web.github.io/</link><description>This is my data science blog.</description><atom:link type="application/rss+xml" href="https://post2web.github.io/rss.xml" rel="self"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:post2web@gmail.com"&gt;Ivelin Angelov&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
</copyright><lastBuildDate>Sun, 21 Jan 2018 05:35:25 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Word2Vec with TensorFlow</title><link>https://post2web.github.io/posts/word2vec-with-tensorflow/</link><dc:creator>Ivelin Angelov</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Word2Vec-with-Skip-Gram-and-TensorFlow"&gt;Word2Vec with Skip-Gram and TensorFlow&lt;a class="anchor-link" href="https://post2web.github.io/posts/word2vec-with-tensorflow/#Word2Vec-with-Skip-Gram-and-TensorFlow"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is a tutorial and a basic example for getting started with word2vec model by &lt;a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;Mikolov et al&lt;/a&gt;. It is used for learning vector representations of words, called "Words Embeddings". For more information about Embeddings, read my previous post.&lt;/p&gt;
&lt;h4 id="The-word2vec-model-can-be-trained-with-two-different-word-representations:"&gt;The word2vec model can be trained with two different word representations:&lt;a class="anchor-link" href="https://post2web.github.io/posts/word2vec-with-tensorflow/#The-word2vec-model-can-be-trained-with-two-different-word-representations:"&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Continuous Bag-of-Words (CBOW)&lt;/b&gt;: predicts target words (e.g. 'mat') from source context words ('the cat sits on the')&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Skip-Gram&lt;/b&gt;: predicts source context-words from the target words&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="Skip-Gram-tends-to-do-better-and-this-tutorial-will-implement-a-word2vec-with-skip-grams."&gt;Skip-Gram tends to do better and this tutorial will implement a word2vec with skip-grams.&lt;a class="anchor-link" href="https://post2web.github.io/posts/word2vec-with-tensorflow/#Skip-Gram-tends-to-do-better-and-this-tutorial-will-implement-a-word2vec-with-skip-grams."&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The goal of the model is to train it's embeddings layer in a way that similar by meaning words are close to each other in their N-dimensional vector representation. The model has two layers: the embeddings layer and a linear layer. Because of the last layer is linear, the distance between embedding vectors for words is linearly related to the distance in the meaning of those words. In other words, we are able to do such mathematical operations with the vectors: &lt;b&gt;[king] - [man] + [woman] ~= [queen]&lt;/b&gt;
&lt;/p&gt;&lt;p&gt;&lt;a href="https://post2web.github.io/posts/word2vec-with-tensorflow/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://post2web.github.io/posts/word2vec-with-tensorflow/</guid><pubDate>Sat, 20 Jan 2018 21:00:07 GMT</pubDate></item><item><title>Embeddings with TensorFlow</title><link>https://post2web.github.io/posts/embeddings-with-tensorflow/</link><dc:creator>Ivelin Angelov</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Embeddings-in-TensorFlow"&gt;Embeddings in TensorFlow&lt;a class="anchor-link" href="https://post2web.github.io/posts/embeddings-with-tensorflow/#Embeddings-in-TensorFlow"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To represent discrete values such as words to a machine learning algorithm, we need to transform every class to a one-hot encoded vector or to an embedding vector.&lt;/p&gt;
&lt;p&gt;Using embeddings for a sparse data often results in more efficient representation as compared to the one-hot encoding approach. For example, a typical vocabulary size for NLP problems is usually from 20,000 to 200,000 unique words. It will be very inefficient to represent every word by a vector of thousands of 0s and only one 1.&lt;/p&gt;
&lt;p&gt;Embeddings can also be "trained" by an optimizer to have different similarities which could represent semantic similarities between words. For example, a model using trained embeddings could predict a test dataset with words unseen before in the training dataset and still have a logical inference based on similar words before seen when training.&lt;/p&gt;
&lt;p&gt;In this post, I'll show and describe use cases of embeddings with Python and TensorFlow.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://post2web.github.io/posts/embeddings-with-tensorflow/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://post2web.github.io/posts/embeddings-with-tensorflow/</guid><pubDate>Sat, 20 Jan 2018 00:00:40 GMT</pubDate></item><item><title>Jupyter Blogging</title><link>https://post2web.github.io/posts/jupyter-blogging/</link><dc:creator>Ivelin Angelov</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Jupyter-Blogging-in-5-minutes."&gt;Jupyter Blogging in 5 minutes.&lt;a class="anchor-link" href="https://post2web.github.io/posts/jupyter-blogging/#Jupyter-Blogging-in-5-minutes."&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Q: What is Jupyter Blogging?&lt;br&gt;
A: Blogging with jupyter notebooks.&lt;/p&gt;
&lt;p&gt;Q: Why Jupyter Blogging?&lt;br&gt;
A: As a Data Scientists, I use jupyter to create notebooks with code, equations, visualizations, documentation, etc. "Jupyter Blogging" allows me to share those notebooks with the world without any additional work.&lt;/p&gt;
&lt;p&gt;Q: How is this achieved?&lt;br&gt;
A: &lt;a href="http://jupyter.org"&gt;Jupyter Notebook&lt;/a&gt; + 
&lt;a href="https://pages.github.com"&gt;Github Pages&lt;/a&gt; + 
&lt;a href="https://getnikola.com"&gt;Nikola&lt;/a&gt; = Jupyter Blogging.&lt;/p&gt;
&lt;p&gt;This tutorial will give you basic instructions for setting up a minimum blog powered by jupyter notebooks. This tutorial is written on a jupyter notebook.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://post2web.github.io/posts/jupyter-blogging/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://post2web.github.io/posts/jupyter-blogging/</guid><pubDate>Sun, 14 Jan 2018 23:01:33 GMT</pubDate></item></channel></rss>